[[alerts-ui-monitor]]
[role="xpack"]
== Monitor and troubleshoot rule executions

The Rules page offers several ways to gain insight into the status of your detection rules:

* <<rule-monitoring-tab, Rule Monitoring tab>> — The current state of all detection rules and their most recent executions. Go to the *Rule Monitoring* tab to get an overview of which rules are running, how long they're taking, and if they're having any trouble.

* <<rule-execution-logs, Rule execution logs>> — Historical data for a single detection rule's executions over time. Consult the rule execution logs to understand how a particular rule is running and whether it's creating the alerts you expect.

Refer to the <<troubleshoot-signals>> section below for strategies on using these tools.

[float]
[[rule-monitoring-tab]]
=== Rule Monitoring tab

To view a summary of all rule executions, including the most recent failures and execution
times, select the *Rule Monitoring* tab on the *Rules* page (*Detect* ->
*Rules* -> *Rule Monitoring*).

[role="screenshot"]
image::images/monitor-table.png[]

On the *Rule Monitoring* tab, you can <<sort-filter-rules, sort and filter rules>> just like you can on the *Rules* tab. 

TIP: By default, you can only sort by the *Rule* and *Enabled* columns on this tab. To sort by any other column, switch on the *Technical preview* toggle above the table. This experimental table view allows advanced sorting capabilities. If you experience performance issues when working with the table, you can turn this setting off.

For detailed information on a rule, the alerts it generated, and associated errors, click on its name in the table. This also allows you to perform the same actions that are available on the <<rules-ui-management, **Rules** tab>>, such as modifying or deleting rules, activating or deactivating rules, exporting or importing rules, and duplicating prebuilt rules.

[float]
[[rule-execution-logs]]
=== Rule execution logs

Each detection rule execution is logged, including its success or failure, any warning or error messages, and how long it took to search for data, create alerts, and complete. This can help you troubleshoot a particular rule if it isn't behaving as expected (for example, if it isn't creating alerts or takes a long time to run).

To access a rule's execution log, go to *Detect* -> *Rules*, click the rule's name to open its details, then scroll down and select the *Rule execution logs* tab.

[role="screenshot"]
image::images/rule-execution-logs.png[Rule execution logs table]

You can hover over each column heading to display a tooltip about that column's data. Click a column heading to sort the table by that column.

Use these controls to filter what's included in the logs table:

* The *Status* drop-down filters the table by rule execution status: 
** *Succeeded*: The rule was able to complete its defined search. This doesn't necessarily mean it generated an alert, just that it ran without error.
** *Failed*: The rule encountered an error that prevented it from running. For example, a machine learning rule whose corresponding machine learning job wasn't running.
** *Partial failure*: Nothing prevented the rule from running, but it still might not have returned the expected results. For example, a custom query rule tried to search an index pattern that couldn't be found in {es}.

* The date and time picker sets the time range of rule executions included in the table. This is separate from the global date and time picker at the top of the rule details page.

* The *Show metrics columns* toggle includes more or less data in the table, pertaining to the timing of each rule execution.

* The *Actions* column allows you to show alerts generated from a given rule execution. Click the filter icon (image:images/filter-icon.png[Filter icon,18,17]) to create a global search filter based on the rule execution's ID value. This replaces any previously applied filters, changes the global date and time range to 24 hours before and after the rule execution, and displays a confirmation notification. You can revert this action by clicking *Restore previous filters* in the notification.

[float]
[[troubleshoot-signals]]
=== Troubleshoot missing alerts

When a rule fails to run close to its scheduled time, some alerts may be
missing. There are a number of ways to try to resolve this issue:

* <<troubleshoot-gaps>>
* <<troubleshoot-ingestion-pipeline-delay>>
* <<ml-job-compatibility>>

You can also use Task Manager in {kib} to troubleshoot background tasks and processes that may be related to missing alerts:

* {kibana-ref}/task-manager-health-monitoring.html[Task Manager health monitoring]
* {kibana-ref}/task-manager-troubleshooting.html[Task Manager troubleshooting]

[float]
[[troubleshoot-gaps]]
==== Troubleshoot gaps

If you see values in the Gaps column in the Rule Monitoring table or on the Rule details page
for a small number of rules, you can increase those rules'
Additional look-back time (*Detect* -> *Rules* -> the rule's *All actions* menu (*...*) -> *Edit rule settings* -> *Schedule* -> *Additional look-back time*).

It's recommended to set the `Additional look-back time` to at
least 1 minute. This ensures there are no missing alerts when a rule doesn't
run exactly at its scheduled time.

{elastic-sec} prevents duplication. Any duplicate alerts that are discovered during the
`Additional look-back time` are _not_ created.

NOTE: If the rule that experiences gaps is an indicator match rule, see <<tune-indicator-rules, how to tune indicator match rules>>. Also please note that {es-sec} provides <<support-indicator-rules, limited support for indicator match rules>>.

If you see gaps for numerous rules:

* If you restarted {kib} when many rules were activated, try deactivating them
and then reactivating them in small batches at staggered intervals. This
ensures {kib} does not attempt to run all the rules at the same time.
* Consider adding another {kib} instance to your environment.

[float]
[[troubleshoot-ingestion-pipeline-delay]]
==== Troubleshoot ingestion pipeline delay

Even if your rule runs at its scheduled time, there might still be missing alerts if your ingestion pipeline delay is greater than your rule interval + additional look-back time. Prebuilt rules have a minimum interval + additional look-back time of 6 minutes in {stack} version >=7.11.0. To avoid missed alerts for prebuilt rules, use caution to ensure that ingestion pipeline delays remain below 6 minutes.

In addition, use caution when creating custom rule schedules to ensure that the specified interval + additional look-back time is greater than your deployment's ingestion pipeline delay.

You can reduce the number of missed alerts due to ingestion pipeline delay by specifying the `Timestamp override` field value to `event.ingested` in <<rule-ui-advanced-params, advanced settings>> during rule creation or editing. The detection engine uses the value from the `event.ingested` field as the timestamp when executing the rule.

For example, say an event occurred at 10:00 but wasn't ingested into {es} until 10:10 due to an ingestion pipeline delay. If you created a rule to detect that event with an interval + additional look-back time of 6 minutes, and the rule executes at 10:12, it would still detect the event because the `event.ingested` timestamp was from 10:10, only 2 minutes before the rule executed and well within the rule's 6-minute interval + additional look-back time.

[role="screenshot"]
image::images/timestamp-override.png[]

[float]
[[ml-job-compatibility]]
==== Troubleshoot missing alerts for {ml} jobs

{ml} detection rules use {ml} jobs that in turn have dependencies on data fields populated by the {beats} and {agent} integrations. In version 8.3.0, new {ml} jobs (prefixed with `V3`) were released to operate on the ECS fields available at that time. If you're using multiple versions of {beats} or {agent} (from both before and after 8.3.0), you may need to duplicate or create new {ml} rules that specify the new ML `V3` jobs and enable them to run alongside your existing rules, to ensure continued coverage from rules currently using `V1` or `V2` jobs.

* If you have only 8.2 or earlier versions of {beats} and {agent}, you can continue using the same `V1` and `V2` {ml} jobs and the existing prebuilt {ml} rules. 

* If you have only 8.3 or later versions of {beats} and {agent}, you can use the latest `V3` {ml} jobs. To generate alerts for anomalies using these jobs, duplicate the existing {ml} rules and reconfigure them to use the `V3` {ml} jobs.

* If you have a mix of old and new versions of {beats} or a mix of {beats} and {agent} integrations, you can use both the old and new {ml} jobs. To generate alerts for anomalies using the new jobs, duplicate the existing {ml} rules and reconfigure them to use the `V3` {ml} jobs.

* If you have a non-Elastic data shipper that gathers ECS-compatible Windows
events, use the `Security:Windows` {ml} jobs. To generate alerts for anomalies using these jobs, duplicate the existing {ml} rules and reconfigure them to use the `V3` {ml} jobs.

If you're duplicating prebuilt {ml} rules to generate alerts using the new `V3` {ml} jobs,
the following rules are affected. Use the new {ml} job listed for each rule:

* <<unusual-linux-network-port-activity>>: `v3_linux_anomalous_network_port_activity`

* <<anomalous-process-for-a-linux-population>>: `v3_linux_anomalous_process_all_hosts`

* <<unusual-linux-username>>: `v3_linux_anomalous_user_name`

* <<unusual-linux-process-calling-the-metadata-service>>: `v3_linux_rare_metadata_process`

* <<unusual-linux-user-calling-the-metadata-service>>: `v3_linux_rare_metadata_user`

* <<unusual-process-for-a-linux-host>>: `v3_rare_process_by_host_linux`

* <<unusual-process-for-a-windows-host>>: `v3_rare_process_by_host_windows`

* <<unusual-windows-network-activity>>: `v3_windows_anomalous_network_activity`

* <<unusual-windows-path-activity>>: `v3_windows_anomalous_path_activity`

* <<anomalous-windows-process-creation>>: `v3_windows_anomalous_process_creation`

* <<anomalous-process-for-a-windows-population>>: `v3_windows_anomalous_process_all_hosts` 

* <<unusual-windows-username>>: `v3_windows_anomalous_user_name`

* <<unusual-windows-process-calling-the-metadata-service>>: `v3_windows_rare_metadata_process`

* <<unusual-windows-user-calling-the-metadata-service>>: `v3_windows_rare_metadata_user`
